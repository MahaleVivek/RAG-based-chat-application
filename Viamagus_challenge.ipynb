{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7fFNOEznxbpPuaOwsTQRp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahaleVivek/RAG-based-chat-application/blob/main/Viamagus_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMJaVhI1Ct4S",
        "outputId": "4a56d357-b409-42ab-b8df-ba36cd1bf9b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.35.13)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (4.1.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.7.4)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.18.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers) (12.5.82)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install openai pinecone-client sentence-transformers tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add all impports here\n",
        "import os\n",
        "import time\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "tk8TR8zeDY-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable definitions\n",
        "OPENAI_API_KEY = \"<YOUR_OPENAI_API_KEY>\"\n",
        "PINECONE_API_KEY = \"<YOUR_PINECONE_API_KEY>\"\n",
        "PINECONE_CLOUD = '<SELECTED_PINECONE_CLOUD>'\n",
        "PINECONE_REGION = '<SELECTED_PINECONE_REGION>'"
      ],
      "metadata": {
        "id": "fhLytpDdEHWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize OpenAI client\n",
        "client = OpenAI(\n",
        "    api_key=OPENAI_API_KEY,\n",
        ")\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_CLOUD)"
      ],
      "metadata": {
        "id": "1GAqW9ugFaZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_WuTHKXhOA0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 225\n",
        "PINECONE_INDEX_NAME = 'chat-history'"
      ],
      "metadata": {
        "id": "I_jwchokGvR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the conversation history\n",
        "\n",
        "history = [\n",
        " \"1: User: Hi there! How are you doing today? | Bot: Hello! I'm doing great, thank you! How can I assist you today?\",\n",
        " \"2: User: What's the weather like today in New York? | Bot: Today in New York, it's sunny with a slight chance of rain.\",\n",
        " \"3: User: Great! Do you have any good lunch suggestions? | Bot: Sure! How about trying a new salad recipe?\",\n",
        " \"4: User: That sounds healthy. Any specific recipes? | Bot: You could try a quinoa salad with avocado and chicken.\",\n",
        " \"5: User: Sounds delicious! I'll try it. What about dinner? | Bot: For dinner, you could make grilled salmon with vegetables.\",\n",
        " \"6: User: Thanks for the suggestions! Any dessert ideas? | Bot: How about a simple fruit salad or yogurt with honey?\",\n",
        " \"7: User: Perfect! Now, what are some good exercises? | Bot: You can try a mix of cardio and strength training exercises.\",\n",
        " \"8: User: Any specific recommendations for cardio? | Bot: Running, cycling, and swimming are all excellent cardio exercises.\",\n",
        " \"9: User: I'll start with running. Can you recommend any books? | Bot: 'Atomic Habits' by James Clear is a highly recommended book.\",\n",
        " \"10: User: I'll check it out. What hobbies can I take up? | Bot: You could explore painting, hiking, or learning a new instrument.\",\n",
        " \"11: User: Hiking sounds fun! Any specific trails? | Bot: There are great trails in the Rockies and the Appalachian Mountains.\",\n",
        " \"12: User: I'll plan a trip. What about indoor activities? | Bot: Indoor activities like reading, cooking, or playing board games.\",\n",
        " \"13: User: Nice! Any good board games? | Bot: Settlers of Catan and Ticket to Ride are both excellent choices.\",\n",
        " \"14: User: I'll try them out. Any movie recommendations? | Bot: 'Inception' and 'The Matrix' are must-watch movies.\",\n",
        " \"15: User: I love those movies! Any TV shows? | Bot: 'Breaking Bad' and 'Stranger Things' are very popular.\",\n",
        " \"16: User: Great choices! What about podcasts? | Bot: 'How I Built This' and 'The Daily' are very informative.\",\n",
        " \"17: User: Thanks! What are some good travel destinations? | Bot: Paris, Tokyo, and Bali are amazing travel spots.\",\n",
        " \"18: User: I'll add them to my list. Any packing tips? | Bot: Roll your clothes to save space and use packing cubes.\",\n",
        " \"19: User: That's helpful! What about travel insurance? | Bot: Always get travel insurance for safety and peace of mind.\",\n",
        " \"20: User: Thanks for the tips! Any last advice? | Bot: Just enjoy your journey and make the most out of your experiences.\"\n",
        "]"
      ],
      "metadata": {
        "id": "tiwARoBKHISf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=device)"
      ],
      "metadata": {
        "id": "jNnZcTp-IJEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "existing_indexes = [\n",
        "index_info[\"name\"] for index_info in pc.list_indexes()\n",
        "]\n",
        "\n",
        "existing_indexes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfMLsNMrN7nc",
        "outputId": "7793161a-c7d4-4249-fa2f-221b82868725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to add embeddings to Pinecone\n",
        "def add_embeddings_to_pinecone(history, index_name='chat-history'):\n",
        "    \"\"\"\n",
        "    This function should:\n",
        "    1. Encode each message in the history using a transformer model.\n",
        "    2. Include the message number as part of the embedding.\n",
        "    3. Create a Pinecone index if it does not exist.\n",
        "    4. Upsert the encoded messages into the Pinecone index.\n",
        "    \"\"\"\n",
        "    # check if index already exists (it shouldn't if this is first time)\n",
        "    if PINECONE_INDEX_NAME not in pc.list_indexes().names():\n",
        "        print(f\"\\nTrue, {PINECONE_INDEX_NAME} is not present so creating one...\")\n",
        "        # if does not exist, create index\n",
        "        pc.create_index(\n",
        "            PINECONE_INDEX_NAME,\n",
        "            dimension=384,  # dimensionality of minilm\n",
        "            metric='cosine',\n",
        "            spec=ServerlessSpec(\n",
        "                cloud=PINECONE_CLOUD,\n",
        "                region=PINECONE_REGION\n",
        "                )\n",
        "            )\n",
        "        # wait for index to be initialized\n",
        "        while not pc.describe_index(PINECONE_INDEX_NAME).status['ready']:\n",
        "            time.sleep(1)\n",
        "\n",
        "    # Connect to the index\n",
        "    index = pc.Index(PINECONE_INDEX_NAME)\n",
        "\n",
        "    # Prepare the data to upsert\n",
        "    data = []\n",
        "    for i, entry in enumerate(history, start=1):\n",
        "        _, message = entry.split('| Bot:', 1)\n",
        "        embedding = model.encode(message.strip()).tolist()\n",
        "        data.append((str(i), embedding, {'text': message.strip()}))\n",
        "\n",
        "\n",
        "    # Upsert the data\n",
        "    index.upsert(data)"
      ],
      "metadata": {
        "id": "gkUL7g1HHkT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_embeddings_to_pinecone(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGkrfHUsUCp6",
        "outputId": "4c2de1e4-30fc-405b-ec28-898521d9fd25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "True, chat-history is not present so creating one...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to retrieve relevant history\n",
        "def retrieve_relevant_history(query, index_name='chat-history', top_k=5):\n",
        "    \"\"\"\n",
        "    This function should:\n",
        "    1. Encode the query using a transformer model.\n",
        "    2. Query the Pinecone index with the encoded query to retrieve the\n",
        "       most relevant messages.\n",
        "    3. Return the relevant messages.\n",
        "    \"\"\"\n",
        "    # Encode the query\n",
        "    query_embedding = model.encode(query).tolist()\n",
        "\n",
        "    # Connect to the index\n",
        "    index = pc.Index(PINECONE_INDEX_NAME)\n",
        "\n",
        "    # Query the index\n",
        "    result = index.query(vector=query_embedding, include_metadata=True, top_k=top_k)\n",
        "\n",
        "    # Extract relevant messages from the query result\n",
        "    relevant_messages = [match['metadata']['text'] for match in result['matches']]\n",
        "\n",
        "    return relevant_messages"
      ],
      "metadata": {
        "id": "dLfJabc5JkAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JH0X4ZfmT__l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_prompt(test_prompt, history, index_name='chat-history', max_tokens=2048):\n",
        "    \"\"\"\n",
        "    This function should:\n",
        "    1. Retrieve relevant history messages using the RAG mechanism.\n",
        "    2. Combine the retrieved messages with the test prompt.\n",
        "    3. Ensure the combined prompt does not exceed the maximum token\n",
        "       limit.\n",
        "    4. Return the combined prompt and the context referred.\n",
        "    \"\"\"\n",
        "    # Retrieve relevant history messages\n",
        "    relevant_history = retrieve_relevant_history(test_prompt, index_name=index_name)\n",
        "\n",
        "    # Combine the retrieved messages with the test prompt\n",
        "    combined_prompt = \"\\n\\n\".join(relevant_history) + \"\\n\\n\" + test_prompt\n",
        "\n",
        "    # Tokenize the combined prompt\n",
        "    encoding = tiktoken.get_encoding('cl100k_base')  # Replace with the appropriate model name if different\n",
        "    encoded_prompt = encoding.encode(combined_prompt)\n",
        "    encoded_prompt = str(encoded_prompt)\n",
        "\n",
        "    # Convert the string representation of the list to an actual list\n",
        "    encoded_prompt_list = eval(encoded_prompt)\n",
        "\n",
        "    # Ensure the combined prompt does not exceed the maximum token limit\n",
        "    if len(encoded_prompt_list) > max_tokens:\n",
        "        # Truncate the history messages if needed\n",
        "        while len(encoded_prompt_list) > max_tokens:\n",
        "            relevant_history.pop(0)  # Remove the oldest message\n",
        "            combined_prompt = \"\\n\\n\".join(relevant_history) + \"\\n\\n\" + test_prompt\n",
        "            encoded_prompt = encoding.encode(combined_prompt)\n",
        "\n",
        "    # Return the combined prompt and the context referred\n",
        "    return combined_prompt, relevant_history"
      ],
      "metadata": {
        "id": "oJ-5Jh5wKGyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to test the final prompt\n",
        "def test_final_prompt():\n",
        "    \"\"\"\n",
        "    This function should:\n",
        "    1. Define the final test prompt.\n",
        "    2. Prepare the prompt using the prepare_prompt function.\n",
        "    3. Call OpenAI to generate a response.\n",
        "    4. Print the final test prompt, the chat history context referred to, and the final response.\n",
        "    5. Validate manually if the final response references the correct context.\n",
        "    \"\"\"\n",
        "    final_test_prompt = \"Do you think it will help me stay fit?\"\n",
        "\n",
        "    # Prepare the prompt using the prepare_prompt function\n",
        "    prepared_prompt, context_referred = prepare_prompt(final_test_prompt, history)\n",
        "\n",
        "    # Get the chat completion response from OpenAI\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\n\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Context: {context_referred}\\n\\n---\\n\\nQuestion: {final_test_prompt}\\nAnswer:\"}\n",
        "            ],\n",
        "        max_tokens=MAX_TOKENS,\n",
        "        temperature=0,\n",
        "        n=1,\n",
        "        stop=None\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFinal Test Prompt: {final_test_prompt}\")\n",
        "    print(f\"\\nContext Referred: {context_referred}\")\n",
        "    print(f\"\\nFinal Test Prompt Response: {response.choices[0].message.content}\")"
      ],
      "metadata": {
        "id": "BqTiXjrGKVOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_final_prompt()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lddjrGyVKpFs",
        "outputId": "854e2536-7cf2-4ff6-b087-05e8407dd391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Test Prompt: Do you think it will help me stay fit?\n",
            "\n",
            "Context Referred: ['You can try a mix of cardio and strength training exercises.', 'Running, cycling, and swimming are all excellent cardio exercises.', 'Roll your clothes to save space and use packing cubes.', 'Sure! How about trying a new salad recipe?', 'Just enjoy your journey and make the most out of your experiences.']\n",
            "\n",
            "Final Test Prompt Response: Yes, trying a mix of cardio and strength training exercises like running, cycling, and swimming can help you stay fit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SRqI7N2jqVZ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}